{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python для сбора данных\n",
    "\n",
    "*Алла Тамбовцева*\n",
    "\n",
    "## Моделирование с `gensim`. Модель Doc2Vec и LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные из файла *articles.csv* и приведем текст к нормальной форме, используя функции, написанные самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/allatambov/Py-programming-3/master/add/articles.csv', \n",
    "                 encoding= 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нижний регистр и удаление пунктуации\n",
    "import string\n",
    "def normalize(x):\n",
    "    to_remove = string.punctuation + '«»—'\n",
    "    translator = str.maketrans('', '', to_remove)\n",
    "    res = x.translate(translator)\n",
    "    res = res.lower()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаление русских стоп-слов и создание списка слов\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def filter_words(text, lang = 'russian'):\n",
    "    \n",
    "    wordsFiltered = []\n",
    "    stopWords = set(stopwords.words(lang))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_norm</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Что такое сверхтекучесть? Нам известно одно ее...</td>\n",
       "      <td>что такое сверхтекучесть нам известно одно ее ...</td>\n",
       "      <td>[такое, сверхтекучесть, нам, известно, одно, п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Тут возникают новые вопросы о том, как вообще ...</td>\n",
       "      <td>тут возникают новые вопросы о том как вообще м...</td>\n",
       "      <td>[возникают, новые, вопросы, вообще, сравнить, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Еще одно интересное употребление редупликации ...</td>\n",
       "      <td>еще одно интересное употребление редупликации ...</td>\n",
       "      <td>[одно, интересное, употребление, редупликации,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Как устроены эти редупликации типа «маслице-фи...</td>\n",
       "      <td>как устроены эти редупликации типа маслицефига...</td>\n",
       "      <td>[устроены, редупликации, типа, маслицефигаслиц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Курс «Расстройства личности: от паранойи и ист...</td>\n",
       "      <td>курс расстройства личности от паранойи и истер...</td>\n",
       "      <td>[курс, расстройства, личности, паранойи, истер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Сопротивление соблазну. В экономических термин...</td>\n",
       "      <td>сопротивление соблазну в экономических термина...</td>\n",
       "      <td>[сопротивление, соблазну, экономических, терми...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>В этом смысле такое картирование может быть ос...</td>\n",
       "      <td>в этом смысле такое картирование может быть ос...</td>\n",
       "      <td>[смысле, такое, картирование, осуществлено, лю...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Существует широко распространенное убеждение, ...</td>\n",
       "      <td>существует широко распространенное убеждение ч...</td>\n",
       "      <td>[существует, широко, распространенное, убежден...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Что такое сверхтекучесть? Нам известно одно ее...   \n",
       "1  Тут возникают новые вопросы о том, как вообще ...   \n",
       "2  Еще одно интересное употребление редупликации ...   \n",
       "3  Как устроены эти редупликации типа «маслице-фи...   \n",
       "4  Курс «Расстройства личности: от паранойи и ист...   \n",
       "5  Сопротивление соблазну. В экономических термин...   \n",
       "6  В этом смысле такое картирование может быть ос...   \n",
       "7  Существует широко распространенное убеждение, ...   \n",
       "\n",
       "                                           text_norm  \\\n",
       "0  что такое сверхтекучесть нам известно одно ее ...   \n",
       "1  тут возникают новые вопросы о том как вообще м...   \n",
       "2  еще одно интересное употребление редупликации ...   \n",
       "3  как устроены эти редупликации типа маслицефига...   \n",
       "4  курс расстройства личности от паранойи и истер...   \n",
       "5  сопротивление соблазну в экономических термина...   \n",
       "6  в этом смысле такое картирование может быть ос...   \n",
       "7  существует широко распространенное убеждение ч...   \n",
       "\n",
       "                                               words  \n",
       "0  [такое, сверхтекучесть, нам, известно, одно, п...  \n",
       "1  [возникают, новые, вопросы, вообще, сравнить, ...  \n",
       "2  [одно, интересное, употребление, редупликации,...  \n",
       "3  [устроены, редупликации, типа, маслицефигаслиц...  \n",
       "4  [курс, расстройства, личности, паранойи, истер...  \n",
       "5  [сопротивление, соблазну, экономических, терми...  \n",
       "6  [смысле, такое, картирование, осуществлено, лю...  \n",
       "7  [существует, широко, распространенное, убежден...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# применяем\n",
    "df['text_norm'] = df.text.apply(normalize)\n",
    "df['words'] = df.text_norm.apply(filter_words)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec c  `gensim`\n",
    "\n",
    "Задача метода *Doc2Vec*, как следует из названия, – представить документы (тексты) в виде числовых векторов. Для того, чтобы понять, как устроен перевод документов в векторы, рассмотрим простейший пример.\n",
    "\n",
    "У нас есть три текста, которые состоят всего из двух слов (двумерный случай легко оказать на графике).\n",
    "\n",
    "*Текст 1:*\n",
    "\n",
    "    бутявка кролик бутявка бутявка бутявка бутявка \n",
    "\n",
    "*Текст 2:*\n",
    "\n",
    "    кролик кролик бутявка кролик\n",
    "    \n",
    "*Текст 3:*\n",
    "\n",
    "    бутявка кролик кролик бутявка\n",
    "    \n",
    "Посчитаем, сколько раз в текстах содержатся слова *бутявка* и *кролик*. Построим матрицу (таблицу частот):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>бутявка</th>\n",
       "      <th>кролик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Текст1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Текст2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Текст3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        бутявка  кролик\n",
       "Текст1        5       1\n",
       "Текст2        1       3\n",
       "Текст3        2       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame([[5, 1, 2], [1, 3, 2]]).T\n",
    "d.columns = ['бутявка', 'кролик']\n",
    "d.index = ['Текст1', 'Текст2', 'Текст3']\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь скажем, что частоты в строках – это координаты вектора, соответствующего определенному документу. Так, вектор для первого текста имеет координаты (5, 1), для второго – (1, 3) и для третьего – (2, 2). Осталось представить эти векторы в пространстве слов, которые есть в нашем словаре (их всего два). Другими словами, *бутявка* и *кролик* будут осями координат, вдоль которых будут располагаться векторы документов. Посмотрим на картинку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(2,2.5,'текст3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = np.array([[5, 1],[1, 3],[2, 2]])\n",
    "origin = [0], [0] \n",
    "plt.quiver(*origin, V[:, 0], V[:, 1], angles='xy', scale_units='xy', scale=1)\n",
    "plt.xlim(0, 8)\n",
    "plt.ylim(0, 5)\n",
    "plt.xlabel('бутявка')\n",
    "plt.ylabel('кролик')\n",
    "plt.text(4, 1.5, 'текст1')\n",
    "plt.text(1, 3.5, 'текст2')\n",
    "plt.text(2, 2.5, 'текст3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какую информацию мы можем извлечь из этого графика? Во-первых, то, что слова *бутявка* и *кролик* в данной модели считаются несвязанными (оси перпендикулярны друг другу). Во-вторых, видно, что текст 1 в большей степени посвящен бутявкам, так как соответствующий вектор ближе к оси `x`. В-третьих, видно, что текст 2 больше о кроликах, так как угол между этим вектором и осью `y` наименьший. Текст 3 находится «посередине», то есть он в равной степени и о бутявках, и о кроликах. \n",
    "\n",
    "Близость между документами можно определять по углу между соответствующими векторами: чем меньше угол, тем ближе тексты к друг другу по смыслу. На основе этой идеи строится кластеризация текстов: считается косинус угла между векторами, соответствующими каждой паре документов, затем на основе [косинусной меры близости](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C) вычисляется расстояние, и далее полученное расстояние можно использовать для реализации кластерного анализа.\n",
    "\n",
    "В общем случае, когда в словаре $N$ слов, а документов $m$, эти $m$ векторов располагаются в $N$-мерном пространстве, и, хотя представить себе это невозможно, определить углы между векторами и посчитать косинусную близость можно достаточно легко (правда, при больших $N$ и $m$ небыстро). Кроме того, в более реалистичных задачах частота, с которой встречается каждое слово, определяется иначе. Вместо обычной частоты считается [мера](https://ru.wikipedia.org/wiki/TF-IDF) *tf-idf* (*term frequency - inverse document frequency*), которая позволяет оценить важность слова в контексте документа, а не просто число его вхождений в текст. Другими словами, эта мера помогает снизить значимость часто встречаемых слов, если они не несут в себе особенной информации о документе, и, наоборот, повысить значимость более редких слов, которые являются характерными для данного текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрали саму идею Doc2Vec, можно приступить к его реализации. Импортируем класс `Doc2Vec` из модуля `models` библиотеки `gensim` и класс `TaggedDocument` для создания корпуса (набора документов). Библиотека `gensim` – популярная [библиотека](https://radimrehurek.com/gensim/) для машинного обучения и, прежде всего, для текстового моделирования (Word2Vec, Doc2Vec, LDA, LSA и проч.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список текстов, чтобы на его основе составить корпус, объект класса `TaggedDocument`, с обычным списком или словарем модель Doc2Vec работать не будет! Корпус ‒ это набор пар *id документа-текст документа*. Поскольку тексты у нас учебные, без заголовков и меток, в качестве *id* возьмем порядковый номер текста в таблице (через *enumerate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = list(df.words)\n",
    "\n",
    "docs = []\n",
    "for i, text in enumerate(Texts):\n",
    "    docs.append(TaggedDocument(text, [i])) # тэг обязательно должен быть списком, поэтому [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['такое', 'сверхтекучесть', 'нам', 'известно', 'одно', 'проявление', 'способность', 'вещества', 'течь', 'трения', 'возьмете', 'ведро', 'гелия', 'поместите', 'любую', 'емкость', 'можете', 'двигать', 'вверх', 'вниз', 'стенками', 'емкости', 'возникнет', 'никакого', 'трения', 'сверхтекучесть', 'открыта', 'гелии4', 'объяснена', 'помощи', 'конденсата', 'бозе', 'эйнштейна', 'атомы', 'гелия4', 'существуют', 'форме', 'бозонов', 'могут', 'формировать', 'микроскопическое', 'квантовое', 'состояние', 'конденсат', 'бозе', 'эйнштейна', 'обычно', 'конденсаты', 'бозе', 'эйнштейна', 'это', 'сверхтекучие', 'материалы', 'менее', 'следует', 'сводить', 'друг', 'другу', 'например', 'сверхтекучем', 'гелии4', 'сила', 'взаимодействия', 'намного', 'других', 'сверхтекучих', 'материалах', 'поэтому', 'общая', 'картина', 'немного', 'сложнаяв', 'целом', 'состоянии', 'сверхтекучести', 'частицы', 'складываются', 'микроскопическое', 'квантовое', 'состояние', 'действуют', 'коллективно', 'наталкиваются', 'препятствие', 'рассеиваются', 'смотреть', 'это', 'точки', 'зрения', 'энергии', 'понять', 'любые', 'возбуждения', 'вне', 'этих', 'микроскопических', 'объектов', 'энергетически', 'невозможны', 'соотношение', 'энергией', 'импульсом', 'изменяется', 'таким', 'образом', 'возбуждения', 'возникает', 'этим', 'объясняется', 'факт', 'сверхтекучая', 'жидкость', 'течет', 'сопротивления'], tags=[0]),\n",
       " TaggedDocument(words=['возникают', 'новые', 'вопросы', 'вообще', 'сравнить', 'две', 'строки', 'поступить', 'консервативно', 'признавать', 'правильным', 'переводом', 'который', 'полностью', 'совпадает', 'эталоном', 'рискуем', 'забраковать', 'очень', 'переводов', 'которые', 'будут', 'абсолютно', 'правильными', 'будут', 'точности', 'совпадать', 'эталоном', 'русском', 'языке', 'порядок', 'слов', 'свободный', 'допускает', 'довольно', 'вариаций', 'например', 'предложения', 'завтра', 'еду', 'отпуск', 'завтра', 'еду', 'отпуск', 'смыслу', 'абсолютно', 'идентичны', 'одинаковая', 'синтаксическая', 'структура', 'просто', 'разный', 'порядок', 'слов', 'вместо', 'проверять', 'точное', 'соответствие', 'эталона', 'переводу', 'можем', 'проверять', 'соответствие', 'уровне', 'называемого', 'мешка', 'слов', 'проверять', 'учета', 'позиции', 'слов', 'перевод', 'признан', 'правильным', 'нем', 'просто', 'содержатся', 'те', 'слова', 'эталоне', 'неважно', 'каких', 'позициях', 'это', 'правильно', 'русский', 'язык', 'свободным', 'порядком', 'слов', 'таким', 'свободным', 'перестановок', 'допускает', 'подряд', 'например', 'поменяете', 'местами', 'подлежащее', 'сказуемое', 'это', 'приемлемо', 'поменяете', 'местами', 'предлог', 'существительное', 'которым', 'управляет', 'предложение', 'потеряет', 'смысл'], tags=[1]),\n",
       " TaggedDocument(words=['одно', 'интересное', 'употребление', 'редупликации', 'русском', 'языке', 'связано', 'значением', 'прототипичности', 'нам', 'сказать', 'какойто', 'объект', 'является', 'типичным', 'своего', 'класса', 'обладает', 'теми', 'свойствами', 'которые', 'обычно', 'связываются', 'такими', 'объектами', 'наших', 'представлениях', 'например', 'говорим', 'прямо', 'свадьбасвадьба', '―', 'это', 'значит', 'это', 'значит', 'такая', 'свадьба', 'прототипически', 'должна', 'устроена', 'свадьба', 'тамадой', 'кучей', 'гостей', 'конкурсами', 'лимузином', 'шариками', 'далее', 'например', 'входит', 'трамвай', 'такая', 'бабушкабабушка', '―', 'сразу', 'примерно', 'понятен', 'образ', 'женщины', 'входящей', 'трамвай', 'русском', 'языке', 'бывает', 'эхоредупликация', 'тюркского', 'типа', 'дупликация', 'м', 'вспомнить', 'слова', 'типа', 'шашлыкмашлык', 'гогольмоголь', 'далее', 'очень', 'хотя', 'сказать', 'такого', 'бывает', 'тех', 'русскоязычных', 'областях', 'имеется', 'контакт', 'тюркскими', 'языками', 'понимаем', 'россии', 'распространено', 'очень', 'тюркских', 'языков', 'например', 'чувашии', 'доводилось', 'слышать', 'поезде', 'недовольную', 'реплику', 'это', 'разложили', 'свои', 'сумкимумки', 'получается', 'такая', 'довольно', 'продуктивная', 'модель', 'образования', 'хотя', 'понятно', 'литературном', 'русском', 'языке', 'это', 'всетаки', 'очень', 'привычно'], tags=[2]),\n",
       " TaggedDocument(words=['устроены', 'редупликации', 'типа', 'маслицефигаслице', 'устроены', 'очень', 'интересно', 'фонетической', 'точки', 'зрения', 'берем', 'слово', 'дальше', 'повторить', 'берем', 'бранную', 'часть', 'скажем', 'фиг', 'добавляем', 'кусок', 'начиная', 'ударного', 'гласного', 'далее', '―', 'маслицефигаслице', 'несколько', 'вполне', 'реальных', 'примеров', 'текстов', 'интернете', 'комиссиифигиссии', 'анализыфигализы', 'трендыфигенды', 'бусикифигусики', 'правило', 'строго', 'соблюдается', 'это', 'видно', 'комиссии', 'ударный', 'гласный', 'соответственно', 'комиссиифигиссии', 'это', 'вполне', 'удобно', 'ситуации', 'ударный', 'гласный', 'находится', 'например', 'конце', 'слова', 'какиенибудь', 'дураки', 'этому', 'правилу', '―', 'должно', 'получиться', 'дуракифиги', 'обычно', 'бывает', 'добавляется', 'слог', 'этим', 'например', 'дуракифигаки'], tags=[3]),\n",
       " TaggedDocument(words=['курс', 'расстройства', 'личности', 'паранойи', 'истерии', 'биопсихосоциальной', 'модели', 'состоится', 'академии', 'постнауки', '11', '13', '15', 'сентября', '1900', '2215', 'автор', 'курса', 'илья', 'плужников', 'кандидат', 'психологических', 'наук', 'старший', 'научный', 'сотрудник', 'научного', 'центра', 'психического', 'здоровья', 'российской', 'академии', 'медицинских', 'наук', 'доцент', 'кафедры', 'нейро', 'патопсихологии', 'факультета', 'психологии', 'мгу', 'м', 'ломоносова'], tags=[4]),\n",
       " TaggedDocument(words=['сопротивление', 'соблазну', 'экономических', 'терминах', 'поведение', 'группы', 'характеризовалось', 'динамической', 'несогласованностью', 'сначала', 'предпочтительным', 'относительно', 'б', 'затем', 'приоритет', 'перешел', 'б', 'динамическая', 'несогласованность', 'нередкое', 'явление', 'субботним', 'утром', 'люди', 'заявляют', 'займутся', 'физкультурой', 'будут', 'сидеть', 'телевизором', 'полудню', 'дома', 'диване', 'смотрят', 'футбол', 'объяснить', 'такое', 'поведение', 'случае', 'кешью', 'действуют', 'фактора', 'искушение', 'бездумность', 'понятие', 'искушения', 'занимает', 'человеческие', 'умы', 'меньшей', 'мере', 'времен', 'адама', 'евы', 'теории', 'подталкивания', 'оно', 'также', 'важно', 'подразумеваем', 'говорим', 'соблазнительном', 'искушающем'], tags=[5]),\n",
       " TaggedDocument(words=['смысле', 'такое', 'картирование', 'осуществлено', 'любым', 'механизмом', 'например', 'биологической', 'нейронной', 'сетью', 'аналоговой', 'цифровой', 'сетью', 'физическим', 'контуром', 'такими', 'физическими', 'контурами', 'могут', 'опорнодвигательный', 'аппарат', 'живых', 'существ', 'различные', 'механические', 'структуры', 'например', 'экспериментах', 'моей', 'лаборатории', 'продемонстрировано', 'роботизированный', 'аналог', 'копыта', 'горного', 'козла', 'иметь', 'высокое', 'сопротивление', 'скольжению', 'счет', 'правильной', 'мягкости', 'основных', 'связок', 'сравнению', 'таким', 'копытом', 'котором', 'суставы', 'менее', 'подвижны', 'также', 'заметили', 'эффект', 'меняется', 'зависимости', 'разных', 'условий', 'местности', 'это', 'пример', 'механический', 'контур', 'тела', 'становится', 'важным', 'физическим', 'свойством', 'зависимости', 'условий', 'среды', 'определяющим', 'состояния', 'скольжения', 'количественные', 'характеристики', 'сопротивления', 'скольжению', 'количественные', 'характеристики', 'другом', 'пространстве'], tags=[6]),\n",
       " TaggedDocument(words=['существует', 'широко', 'распространенное', 'убеждение', 'французскую', 'революцию', 'совершил', 'народ', 'которым', 'обычно', 'понимаются', 'крестьянство', 'городской', 'плебс', 'однако', 'комплексное', 'изучение', 'различных', 'типов', 'массовых', 'движений', 'заставляет', 'большой', 'долей', 'осторожности', 'отнестись', 'этому', 'клише', 'обычно', 'ассоциируемые', 'революцией', 'массовые', 'выступления', 'плебса', 'подавляющем', 'своем', 'большинстве', 'преследовали', 'далеко', 'идущих', 'политических', 'целей', 'являлись', 'лишь', 'непосредственной', 'реакцией', 'информационные', 'поводы', 'вызывавшие', 'всплеск', 'коллективных', 'страхов', 'ситуации', 'общего', 'недовольства', 'текущими', 'экономическими', 'трудностями', 'условно', 'революционные', 'вспышки', 'протеста', 'идут', 'какое', 'сравнение', 'своим', 'масштабам', 'продолжительности', 'массовыми', 'народными', 'движениями', 'направленными', 'непосредственно', 'против', 'системы', 'ценностей', 'французской', 'революции', 'движениями', 'захватывавшими', 'целые', 'регионы', 'продолжавшимися', 'годами'], tags=[7])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1a1fd4dd30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим словарь (набор всех слов в тексте с id):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось построить модель, прогнать ее 10 раз (10 «эпох»):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "don't know how to handle uri 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1ba9ed423b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.alpha -= 0.002  # уменьшить learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model.min_alpha = model.alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# Calculate offsets for each worker along with initial doctags (doctag ~ document/line number in a file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_doctags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_offsets_and_start_doctags_for_corpusfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offsets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_doctags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36m_get_offsets_and_start_doctags_for_corpusfile\u001b[0;34m(cls, corpus_file, workers)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mstart_doctags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mcurr_offset_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mprev_filepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mbinary_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'don\\'t know how to handle uri %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: don't know how to handle uri 8"
     ]
    }
   ],
   "source": [
    "model.train(docs, model.corpus_count, epochs=10)\n",
    "#model.alpha -= 0.002  # уменьшить learning rate\n",
    "#model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если Python выдает ошибку, посмотрите, что он пишет. Часто достаточно добавить дополнительные аргументы в `train()`, например, число эпох, равное 10 или еще что-нибудь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не прогонять модель заново при каждом запуске файла (особенно актуально в случае сложных моделей, которые обучаются несколько часов, а то и дней), ее можно сохранить в файл, а потом просто загрузить и продолжить с ней работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('d2v_model') # сохраняем\n",
    "model = Doc2Vec.load('d2v_model') # открываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count # сколько документов в корпусе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на самые близкие к документу с индексом 0 тексты: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.12205836921930313),\n",
       " (7, 0.050673384219408035),\n",
       " (6, 0.03967006132006645),\n",
       " (1, 0.022813543677330017),\n",
       " (3, 0.01584635302424431),\n",
       " (2, -0.024669073522090912),\n",
       " (4, -0.10029617697000504)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar([0]) # от самого близкого до самого далекого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на сами тексты.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'что такое сверхтекучесть нам известно одно ее проявление  способность вещества течь без трения если вы возьмете ведро гелия и поместите его в любую емкость вы можете двигать его вверх и вниз а между стенками емкости не возникнет никакого трения сверхтекучесть была открыта в гелии4 и объяснена при помощи конденсата бозе  эйнштейна потому что атомы гелия4 существуют в форме бозонов и могут формировать микроскопическое квантовое состояние  конденсат бозе  эйнштейна обычно конденсаты бозе  эйнштейна  это и есть сверхтекучие материалы тем не менее их не следует сводить друг к другу например в сверхтекучем гелии4 сила взаимодействия намного больше чем в других сверхтекучих материалах поэтому общая картина немного более сложнаяв целом в состоянии сверхтекучести частицы складываются в микроскопическое квантовое состояние и действуют коллективно если они наталкиваются на препятствие они не рассеиваются если смотреть на это с точки зрения энергии то можно понять что любые возбуждения вне этих микроскопических объектов энергетически невозможны потому что соотношение между энергией и импульсом изменяется таким образом что возбуждения не возникает этим объясняется тот факт что сверхтекучая жидкость течет без сопротивления'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сопротивление соблазну в экономических терминах поведение группы характеризовалось динамической несогласованностью сначала а было предпочтительным относительно б а затем приоритет перешел к б динамическая несогласованность  нередкое явление субботним утром люди заявляют что лучше займутся физкультурой чем будут сидеть перед телевизором к полудню они все еще дома на диване смотрят футбол чем объяснить такое поведение\\nв случае с кешью действуют два фактора искушение и бездумность понятие искушения занимает человеческие умы по меньшей мере со времен адама и евы для теории подталкивания оно также важно что мы подразумеваем когда говорим о соблазнительном искушающем'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_norm[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первый взгляд, документы как-то непохожи. Это очень логично, так как мы обучали модель всего на восьми коротеньких текстах, что явно недостаточно. С другой стороны, если вчитаться, сходство по словам найти можно: и там, и там речь идет о сопротивлении, возбуждении. Для получения более ярких результатов нужно брать более объемный корпус текстов и желательно с более длинными текстами. Но такие модели, скорее всего, будут долго обучаться, и их мы сегодня строить не будем. Можете попробовать сами, нагуглив какой-нибудь корпус новостных статей. Возможно, позже в порыве вдохновения я выложу какие-то результаты на более объемных корпусах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA: латентное размещение Дирихле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдем к тематическому моделированию ‒ распределению текстов по темам (какие темы на основе имеющихся текстов можно выделить, и какие тексты к ним относятся). Один из самых распространенных методов в тематическом моделировании ‒ это латентное размещение Дирихле (*LDA*, *Latent Dirichlet Allocation*). Прежде чем знакомиться с идеей этого метода, имеет смысл обсудить, причем тут Дирихле. \n",
    "\n",
    "Основное предположение LDA заключается в том, что любой текст (документ) ‒ это смесь разных тем. Какая-то тема выражена более ярко, какая-то менее, какая-то совсем не представлена. Удобно представлять себе задачу распределения текстов по темам, проводя аналогию с экзаменационным заданием по иностранному языку, в котором необходимо сопоставить отрывки и заголовки. В каждом отрывке будет по чуть-чуть упоминаться практически каждая тема, связывающая заголовки, но главная тема будет одна. [Распределение Дирихле](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D0%94%D0%B8%D1%80%D0%B8%D1%85%D0%BB%D0%B5) ‒ это непрерывное многомерное распределение. Его можно задать параметром $\\alpha$ ‒ вектором длины $K$ вида ($\\alpha_1$, $\\alpha_2$, $\\dots$, $\\alpha_K$), где все $\\alpha_i$ лежат в пределах от 0 до 1, и сумма всех $\\alpha_i$ равна 1. Если присмотреться, можно заметить, что этот вектор $\\alpha$ хорошо описывает текст! Мы фиксируем $K$ ‒ число тем, которые можно выделить, а затем для каждого текста определяем $\\alpha = (\\alpha_1, \\alpha_2, \\dots, \\alpha_K$), то есть фиксируем вероятности $\\alpha_i$, с которыми каждая тема встречается в документе. Вероятности всегда лежат в пределах от 0 до 1, и их сумма равна 1 (конечно, речь идет о взаимоисключающих событиях).\n",
    "\n",
    "Как происходит обучение модели LDA? Мы фиксируем число тем $K$, исходя из наших содержательных соображений. Для каждого документа создается вектор $\\alpha$ длины $K$, элементы которого (вероятности встретить ту или иную тему) считаются так: в самом начале каждому слову в документе случайным образом присваивается тема, а затем вычисляется доля слов, относящихся к каждой теме. Далее вероятности в векторе $\\alpha$ считаются более разумным способом: на основе посчитанных условных вероятностей: вероятность встретить определенную тему в данном документе ($\\texttt{тема|документ}$) и вероятность встретить определенное слово в рамках данной темы ($\\texttt{слово|тема}$). Подробнее о механизме см. [здесь](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/). На выходе модель выдает список тем и топовые (часто встречающиеся) слова в рамках каждой темы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея метода зафиксирована, давайте реализуем свой LDA! Работать, правда, будем тоже с учебными данными, совсем небольшим корпусом англоязычных текстов, которые были взяты (и сокращены) из [20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/). Можете пройти по ссылке, скачать все 20000 текстов (сами тексты небольшие, распакованный архив займет около 50 Мб) и построить более внятную тематическую модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим тексты из файла csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When they are victimized they are Muslims. Whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But, if you were to discuss the merits of raci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do you restrict your condemnation of racia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you have a strange definition of \"winning\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In article &lt;kmitchelC4wA87.HLz@netcom.com kmi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post\n",
       "0  When they are victimized they are Muslims. Whe...\n",
       "1  But, if you were to discuss the merits of raci...\n",
       "2  Why do you restrict your condemnation of racia...\n",
       "3   Do you have a strange definition of \"winning\"...\n",
       "4   In article <kmitchelC4wA87.HLz@netcom.com kmi..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('https://raw.githubusercontent.com/allatambov/Py-programming-3/master/add/your_turn.csv')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним предварительную обработку текстов (переопределим функцию `filter_words()`, чтобы она убирала английские стоп-слова, а лучше создадим новую с окончанием `_en`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words_en(text, lang = 'english'):\n",
    "    \n",
    "    wordsFiltered = []\n",
    "    stopWords = set(stopwords.words(lang))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['text_norm'] = d.post.apply(normalize)\n",
    "d['words'] = d.text_norm.apply(filter_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем класс `LdaModel`и модуль `corpora` для создания словаря (перечня всех слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список текстов `texts`, на его основе создадим словарь `dict_`. Затем создадим корпус текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(d.words)\n",
    "dict_ = corpora.Dictionary(texts)\n",
    "corpus =  [dict_.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось построить саму модель. Давайте считать, что в нашем наборе текстов присутствует пять тем (примерно так я их и отбирала: политика, религия, медицина, технологии, атеизм)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=5, id2word = dict_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При построении модели очень важно указать аргумент `id2word` и подставить в него созданный на предыдущем шаге словарь. Если этого не сделать, ничего страшного не случится, но тогда вместо самых распространенных слов в пределах каждой темы мы будем видеть их индексы. Конечно, можно найти слово по индексу в словаре, но это неудобно, учитывая, что слов обычно требуется немало.\n",
    "\n",
    "Выведем 5 тем и топовые слова для каждой темы (30 слов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"would\" + 0.005*\"one\" + 0.004*\"look\" + 0.004*\"like\" + 0.004*\"much\" + 0.004*\"lcd\" + 0.003*\"know\" + 0.003*\"people\" + 0.003*\"baseball\" + 0.003*\"board\" + 0.003*\"probably\" + 0.003*\"muslims\" + 0.003*\"dont\" + 0.003*\"since\" + 0.003*\"could\" + 0.003*\"good\" + 0.003*\"also\" + 0.002*\"first\" + 0.002*\"things\" + 0.002*\"believe\" + 0.002*\"im\" + 0.002*\"time\" + 0.002*\"even\" + 0.002*\"racism\" + 0.002*\"well\" + 0.002*\"right\" + 0.002*\"weight\" + 0.002*\"game\" + 0.002*\"bad\" + 0.002*\"use\"'),\n",
       " (1,\n",
       "  '0.006*\"one\" + 0.005*\"like\" + 0.004*\"dram\" + 0.004*\"use\" + 0.004*\"would\" + 0.004*\"health\" + 0.003*\"know\" + 0.003*\"dont\" + 0.003*\"anyone\" + 0.003*\"wont\" + 0.003*\"done\" + 0.003*\"insurance\" + 0.003*\"ive\" + 0.003*\"care\" + 0.002*\"may\" + 0.002*\"think\" + 0.002*\"blood\" + 0.002*\"though\" + 0.002*\"another\" + 0.002*\"private\" + 0.002*\"believe\" + 0.002*\"really\" + 0.002*\"make\" + 0.002*\"good\" + 0.002*\"im\" + 0.002*\"get\" + 0.002*\"cancer\" + 0.002*\"hospitals\" + 0.002*\"problem\" + 0.002*\"abortion\"'),\n",
       " (2,\n",
       "  '0.005*\"one\" + 0.005*\"first\" + 0.004*\"believe\" + 0.004*\"people\" + 0.004*\"two\" + 0.004*\"hit\" + 0.003*\"would\" + 0.003*\"home\" + 0.003*\"game\" + 0.003*\"second\" + 0.003*\"dont\" + 0.003*\"know\" + 0.003*\"runs\" + 0.003*\"like\" + 0.003*\"little\" + 0.003*\"antisemitism\" + 0.003*\"socks\" + 0.002*\"lilac\" + 0.002*\"going\" + 0.002*\"2\" + 0.002*\"wearing\" + 0.002*\"make\" + 0.002*\"3\" + 0.002*\"statements\" + 0.002*\"good\" + 0.002*\"third\" + 0.002*\"win\" + 0.002*\"run\" + 0.002*\"cubs\" + 0.002*\"era\"'),\n",
       " (3,\n",
       "  '0.005*\"one\" + 0.004*\"game\" + 0.004*\"time\" + 0.003*\"home\" + 0.003*\"first\" + 0.003*\"second\" + 0.003*\"runs\" + 0.003*\"would\" + 0.002*\"two\" + 0.002*\"since\" + 0.002*\"hit\" + 0.002*\"win\" + 0.002*\"well\" + 0.002*\"winning\" + 0.002*\"allowed\" + 0.002*\"like\" + 0.002*\"much\" + 0.002*\"third\" + 0.002*\"know\" + 0.002*\"teams\" + 0.002*\"get\" + 0.002*\"though\" + 0.002*\"people\" + 0.002*\"dont\" + 0.002*\"may\" + 0.002*\"got\" + 0.002*\"tax\" + 0.002*\"astros\" + 0.002*\"right\" + 0.002*\"cubs\"'),\n",
       " (4,\n",
       "  '0.007*\"first\" + 0.005*\"game\" + 0.004*\"second\" + 0.004*\"one\" + 0.004*\"home\" + 0.004*\"runs\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"get\" + 0.003*\"two\" + 0.003*\"32\" + 0.003*\"balls\" + 0.002*\"cubs\" + 0.002*\"another\" + 0.002*\"cotton\" + 0.002*\"win\" + 0.002*\"much\" + 0.002*\"hit\" + 0.002*\"weight\" + 0.002*\"gets\" + 0.002*\"thing\" + 0.002*\"make\" + 0.002*\"dont\" + 0.002*\"got\" + 0.002*\"others\" + 0.002*\"schmidt\" + 0.002*\"mattingly\" + 0.002*\"back\" + 0.002*\"muslims\" + 0.002*\"astros\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(5, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В очередной раз убедились, что а) работать с маленьким корпусом текстов бессмысленно, б) нужно лучше \"вычищать\" тексты от стоп-слов, добавляя свои слова в \"черный\" список. Что будет, если мы попробуем выделить больше тем?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus, num_topics=8, id2word = dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"dram\" + 0.006*\"would\" + 0.005*\"know\" + 0.005*\"use\" + 0.004*\"first\" + 0.004*\"seats\" + 0.004*\"home\" + 0.004*\"two\" + 0.003*\"data\" + 0.003*\"game\" + 0.003*\"chip\" + 0.003*\"baseball\" + 0.003*\"runs\" + 0.003*\"second\" + 0.003*\"one\" + 0.003*\"like\" + 0.003*\"anyone\" + 0.003*\"look\" + 0.003*\"power\" + 0.003*\"get\" + 0.003*\"dont\" + 0.003*\"could\" + 0.002*\"may\" + 0.002*\"another\" + 0.002*\"win\" + 0.002*\"handle\" + 0.002*\"1\" + 0.002*\"powerbook\" + 0.002*\"time\" + 0.002*\"picture\"'),\n",
       " (1,\n",
       "  '0.004*\"since\" + 0.004*\"would\" + 0.004*\"people\" + 0.003*\"think\" + 0.003*\"one\" + 0.003*\"time\" + 0.003*\"know\" + 0.003*\"first\" + 0.003*\"board\" + 0.003*\"going\" + 0.003*\"driving\" + 0.003*\"make\" + 0.002*\"like\" + 0.002*\"right\" + 0.002*\"cancer\" + 0.002*\"use\" + 0.002*\"era\" + 0.002*\"get\" + 0.002*\"im\" + 0.002*\"much\" + 0.002*\"well\" + 0.002*\"situation\" + 0.002*\"aux\" + 0.002*\"brain\" + 0.002*\"smoking\" + 0.002*\"good\" + 0.002*\"point\" + 0.002*\"drinking\" + 0.002*\"quite\" + 0.002*\"others\"'),\n",
       " (2,\n",
       "  '0.006*\"one\" + 0.006*\"like\" + 0.004*\"cotton\" + 0.004*\"believe\" + 0.004*\"first\" + 0.004*\"labor\" + 0.003*\"dont\" + 0.003*\"point\" + 0.003*\"would\" + 0.003*\"im\" + 0.003*\"world\" + 0.003*\"good\" + 0.003*\"american\" + 0.003*\"mechanization\" + 0.003*\"hit\" + 0.003*\"morality\" + 0.003*\"makes\" + 0.003*\"slavery\" + 0.003*\"agricultural\" + 0.003*\"get\" + 0.003*\"much\" + 0.003*\"think\" + 0.003*\"ball\" + 0.002*\"blood\" + 0.002*\"production\" + 0.002*\"guy\" + 0.002*\"better\" + 0.002*\"second\" + 0.002*\"many\" + 0.002*\"1\"'),\n",
       " (3,\n",
       "  '0.007*\"one\" + 0.005*\"believe\" + 0.005*\"weight\" + 0.005*\"antisemitism\" + 0.004*\"make\" + 0.004*\"little\" + 0.004*\"people\" + 0.004*\"like\" + 0.003*\"others\" + 0.003*\"didnt\" + 0.003*\"dont\" + 0.003*\"god\" + 0.003*\"also\" + 0.003*\"done\" + 0.003*\"whether\" + 0.003*\"two\" + 0.003*\"statements\" + 0.003*\"wont\" + 0.003*\"almost\" + 0.003*\"course\" + 0.003*\"hitler\" + 0.003*\"foundation\" + 0.003*\"germany\" + 0.003*\"muslims\" + 0.003*\"socks\" + 0.003*\"laid\" + 0.003*\"nazis\" + 0.003*\"going\" + 0.003*\"part\" + 0.002*\"lilac\"'),\n",
       " (4,\n",
       "  '0.005*\"game\" + 0.005*\"first\" + 0.005*\"one\" + 0.004*\"would\" + 0.004*\"two\" + 0.004*\"second\" + 0.003*\"know\" + 0.003*\"home\" + 0.003*\"health\" + 0.003*\"like\" + 0.003*\"runs\" + 0.003*\"hitter\" + 0.003*\"dont\" + 0.003*\"way\" + 0.002*\"well\" + 0.002*\"hit\" + 0.002*\"cubs\" + 0.002*\"teams\" + 0.002*\"third\" + 0.002*\"insurance\" + 0.002*\"baseball\" + 0.002*\"designated\" + 0.002*\"people\" + 0.002*\"right\" + 0.002*\"even\" + 0.002*\"win\" + 0.002*\"32\" + 0.002*\"since\" + 0.002*\"astros\" + 0.002*\"care\"'),\n",
       " (5,\n",
       "  '0.007*\"would\" + 0.006*\"racism\" + 0.005*\"believe\" + 0.005*\"could\" + 0.005*\"religion\" + 0.004*\"one\" + 0.004*\"weight\" + 0.004*\"bad\" + 0.004*\"first\" + 0.004*\"psychological\" + 0.004*\"benefits\" + 0.003*\"results\" + 0.003*\"works\" + 0.003*\"things\" + 0.003*\"well\" + 0.003*\"course\" + 0.003*\"someone\" + 0.003*\"give\" + 0.003*\"like\" + 0.003*\"right\" + 0.003*\"balls\" + 0.003*\"even\" + 0.003*\"done\" + 0.003*\"mean\" + 0.003*\"people\" + 0.003*\"belief\" + 0.003*\"lcd\" + 0.003*\"experiencing\" + 0.002*\"speak\" + 0.002*\"good\"'),\n",
       " (6,\n",
       "  '0.008*\"one\" + 0.006*\"first\" + 0.005*\"game\" + 0.005*\"home\" + 0.005*\"muslims\" + 0.004*\"people\" + 0.003*\"second\" + 0.003*\"dont\" + 0.003*\"islam\" + 0.003*\"back\" + 0.003*\"hit\" + 0.003*\"got\" + 0.003*\"runs\" + 0.003*\"would\" + 0.003*\"get\" + 0.002*\"hard\" + 0.002*\"bad\" + 0.002*\"know\" + 0.002*\"cotton\" + 0.002*\"cubs\" + 0.002*\"32\" + 0.002*\"win\" + 0.002*\"power\" + 0.002*\"another\" + 0.002*\"drive\" + 0.002*\"used\" + 0.002*\"two\" + 0.002*\"team\" + 0.002*\"muslim\" + 0.002*\"good\"'),\n",
       " (7,\n",
       "  '0.004*\"game\" + 0.004*\"would\" + 0.003*\"like\" + 0.003*\"runs\" + 0.003*\"home\" + 0.003*\"one\" + 0.003*\"though\" + 0.003*\"much\" + 0.003*\"second\" + 0.003*\"blood\" + 0.003*\"cancer\" + 0.003*\"since\" + 0.003*\"first\" + 0.003*\"good\" + 0.003*\"another\" + 0.002*\"look\" + 0.002*\"probably\" + 0.002*\"winning\" + 0.002*\"fact\" + 0.002*\"well\" + 0.002*\"cubs\" + 0.002*\"tax\" + 0.002*\"time\" + 0.002*\"astros\" + 0.002*\"last\" + 0.002*\"get\" + 0.002*\"run\" + 0.002*\"dont\" + 0.002*\"insurance\" + 0.002*\"third\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(8, num_words=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наверное, результаты получились чуть более разумным :) По крайней мере, здесь можно выделить темы *религия*, *политика* и *спорт*. Но, конечно, работать с таким малым числом документов несерьезно, этот пример был больше техническим, иллюстрирующим то, каким образом можно предобработать текст, сформировать корпус и обучить модель LDA.\n",
    "\n",
    "Возможно, позже я выложу более реалистичные примеры с LDA на большом объеме данных + дополню модель разными визуализациями, например, облаками слов для каждой темы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
